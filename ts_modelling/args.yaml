# device:
use_gpu: 1
use_multi_gpu: 0
gpu: 0
num_workers: 0

# data & directories
root_path: ./data
data_path: custom.csv

# data and number of epochs for different training tasks
pretrain_data:
  odd_sine: 1
train_head_data:
  odd_sine: 1
finetune_data:
  odd_sine: 1
test_data:
  even_sine

checkpoints: ./checkpoints # where to store model weights
model_name: PTST_train_even_test_odd

# target:
features: S # S = univariate, M = multivariate, MS = multi predict uni
target: signal_1 # name of target column in data if MS

# modelling:
model: PatchTST
pred_len: 96 # 192
seq_len: 168 # 336 # lookback window
enc_in: 1
# model params:
# dims:
e_layers: 1 # 8 # number of stacked encoders
n_heads:  3 # 16
d_model: 12 # 128 # latent space dim
d_ff:  12 #256 # feed forward network

# patching
patch_len: 12
stride: 12
padding_patch: end

# dropout
dropout: 0.2 # dropout in W_P embed layer (before encoder)
fc_dropout: 0.2 # dropout in pretrain head
head_dropout: 0.2

# normalisation
revin: 1 # reversible instance normalisation
affine: 0 # affine transformation in revin
subtract_last: 0

# setting:
decomposition: 0 # decomposition into trend and residual
kernel_size: 25 # moving average window in decomposition
individual: 0

# training
lr: 0.0001  # max learning rate in annealing
lr_scheduler: one_cycle_lr # currently the only supported
lr_pct_start: 0.1 # what percent of max learning rate
loss: MSE # currently supports MSE, L1/MAE

training_task: self_supervised
mask_pct: 0.4

patience: 15
train_epochs: 30
batch_size: 32


# communication
verbose: false # if true then it will print early stopping and learning rate status
batch_log_interval: 100 # the number of batches between status print
epoch_log_interval: 10 # the number of epochs between status print

# unnecessary â€“ these are residuals from dataloader class from FEDformer
embed: timeF
embed_type: 0
freq: h





