# device:
use_gpu: 1
use_multi_gpu: 0
gpu: 0
num_workers: 0

# data & directories
root_path: ./data
data_path: temp_malmo_small.csv

# data and number of epochs for different training tasks
pretrain_data:
  supervised: 0
train_head_data:
  supervised: 0
finetune_data:
  temp_malmo.csv: 1
test_data:
  temp_katterjakk.csv

checkpoints: ./checkpoints # where to store model weights
model_name: TBD

# target:
features: S # S = univariate, M = multivariate, MS = multi predict uni
target: temperature # name of target column in data if MS

# modelling:
model: PatchTST
pred_len: 192
seq_len: 336 # lookback window
enc_in: 1
# model params:
# dims:
e_layers: 3 # number of stacked encoders
n_heads: 16
d_model: 32 # latent space dim
d_ff: 64 # feed forward network

# patching
patch_len: 12
stride: 12
padding_patch: end

# dropout
dropout: 0.2 # dropout in W_P embed layer (before encoder)
fc_dropout: 0.2 # dropout in pretrain head
head_dropout: 0.2

# normalisation
revin: 1 # reversible instance normalisation
affine: 0 # affine transformation in revin
subtract_last: 0

# setting:
decomposition: 0 # decomposition into trend and residual
kernel_size: 25 # moving average window in decomposition
individual: 0

# training
lr: 0.0001  # max learning rate in annealing
lr_scheduler: one_cycle_lr # currently the only supported
lr_pct_start: 0.1 # what percent of max learning rate
loss: MSE # currently supports MSE, L1/MAE

training_task: supervised
mask_pct: 0.4

patience: 5
train_epochs: 1
batch_size: 32


# communication
verbose: true # if true then it will print early stopping and learning rate status
batch_log_interval: 100 # the number of batches between status print
epoch_log_interval: 5 # the number of epochs between status print